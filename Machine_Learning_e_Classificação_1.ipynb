{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUGQtfPXYreG"
      },
      "outputs": [],
      "source": [
        "# features (caracteristicas que x possui, 1 sim, 0 não)\n",
        "# pelo longo?\n",
        "# perna curta?\n",
        "# faz auau?\n",
        "# tem escamas?\n",
        "# tem asas?\n",
        "\n",
        "porco1 = [0, 1, 0]\n",
        "porco2 = [0, 1, 1]\n",
        "porco3 = [1, 1, 0]\n",
        "\n",
        "cachorro1 = [0, 1, 1]\n",
        "cachorro2 = [1, 0, 1]\n",
        "cachorro3 = [1, 1, 1]\n",
        "\n",
        "\n",
        "# 1 => porco, 0 => cachorro\n",
        "treino_x = [porco1, porco2, porco3, cachorro1, cachorro2, cachorro3]\n",
        "treino_y = [1,1,1,0,0,0] # labels / etiqueta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7xbio_ib7Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# features (1 sim, 0 não)\n",
        "# pelo longo?\n",
        "# perna curta?\n",
        "# faz auau?\n",
        "porco1 = [0, 1, 0]\n",
        "porco2 = [0, 1, 1]\n",
        "porco3 = [1, 1, 0]\n",
        "\n",
        "cachorro1 = [0, 1, 1]\n",
        "cachorro2 = [1, 0, 1]\n",
        "cachorro3 = [1, 1, 1]\n",
        "\n",
        "# 1 => porco, 0 => cachorro\n",
        "treino_x = [porco1, porco2, porco3, cachorro1, cachorro2, cachorro3]\n",
        "treino_y = [1,1,1,0,0,0] # labels / etiqueta\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(treino_x, treino_y)\n",
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "     verbose=0)\n",
        "animal_misterioso = [1,1,1]\n",
        "model.predict([animal_misterioso])\n",
        "array([0])\n",
        "misterio1 = [1,1,1]\n",
        "misterio2 = [1,1,0]\n",
        "misterio3 = [0,1,1]\n",
        "\n",
        "teste_x = [misterio1, misterio2, misterio3]\n",
        "teste_y = [0, 1, 1]\n",
        "previsoes = model.predict(teste_x)\n",
        "corretos = (previsoes == teste_y).sum()\n",
        "total = len(teste_x)\n",
        "taxa_de_acerto = corretos/total\n",
        "print(\"Taxa de acerto %.2f\" % (taxa_de_acerto * 100))\n",
        "Taxa de acerto 66.67\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "taxa_de_acerto = accuracy_score(teste_y, previsoes)\n",
        "print(\"Taxa de acerto %.2f\" % (taxa_de_acerto * 100))\n",
        "Taxa de acerto 66.67\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "HG08jON-4bl0",
        "outputId": "02a368b5-3fbd-4cf3-ce90-1bedf747a888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-e3539bba352b>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    Taxa de acerto 66.67\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animal_misterioso = [1,1,1]\n",
        "model.predict([animal_misterioso])"
      ],
      "metadata": {
        "id": "PgvMYZnJFDND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misterio1 = [1,1,1]\n",
        "misterio2 = [1,1,0]\n",
        "misterio3 = [0,1,1]\n",
        "\n",
        "teste x  = [misterio1, misterio2, misterio3]\n",
        "teste_y = [0, 1, 1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nZkQUH_aGve6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[0, 1, 1]"
      ],
      "metadata": {
        "id": "UUtQGi_jHhVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sqir6bm2E2TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes == testes_classes"
      ],
      "metadata": {
        "id": "KQVH8jUMNBpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corretos = (previsoes == testes_classes).sum()\n",
        "#quantidade de True\n",
        "total = len(testes)\n",
        "taxa_de_acerto = corretos/total\n",
        "print(\"Taxa de acerto: \", taxa_de_acerto * 100)"
      ],
      "metadata": {
        "id": "0C2taibwOGWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "taxa_de_acerto = accuracy_score(testes_classes, previsoes)\n",
        "print(\"Taxa de acerto\", taxa_de_acerto * 100)"
      ],
      "metadata": {
        "id": "osXNOuxsOHJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxIYp-btYg5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}